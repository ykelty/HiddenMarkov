{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQkiQk7LNe9q",
        "outputId": "12513d82-f888-4c4a-d3bf-ff4f88202b93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "#Establishing connection to drive for Colab\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FFxvUNCZIzoe"
      },
      "outputs": [],
      "source": [
        "# Import assignment files\n",
        "train = \"/content/drive/My Drive/ColabNotebooks/CSCI544_HW3/train\"\n",
        "dev = \"/content/drive/My Drive/ColabNotebooks/CSCI544_HW3/dev\"\n",
        "test= \"/content/drive/My Drive/ColabNotebooks/CSCI544_HW3/test\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: Vocabulary Creation"
      ],
      "metadata": {
        "id": "L8Eo5dYvEpWc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "R_bCK70hLt04"
      },
      "outputs": [],
      "source": [
        "# Create a Word Count Dictionary\n",
        "word_counts = {}\n",
        "\n",
        "# While reading in the file, add to dictionary\n",
        "with open(train, \"r\", encoding=\"utf-8\") as file:\n",
        "    for row in file:\n",
        "        row = row.strip()\n",
        "        if row:\n",
        "            _, word, _ = row.split(\"\\t\")\n",
        "            if word in word_counts:\n",
        "                word_counts[word] += 1\n",
        "            else:\n",
        "                word_counts[word] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zsZG6ShNQH3V"
      },
      "outputs": [],
      "source": [
        "# Create a vocab dictionary\n",
        "vocab = {\"<unk>\": 0}\n",
        "filtered_counts = {\"<unk>\": 0}\n",
        "index = 1\n",
        "sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MWR_-_iFQULr"
      },
      "outputs": [],
      "source": [
        "for word, count in sorted_words:\n",
        "    if count < 3: # Threshold specified in instructions\n",
        "        filtered_counts[\"<unk>\"] += count  # Add occurrences to <unk>\n",
        "    else:\n",
        "        vocab[word] = index\n",
        "        filtered_counts[word] = count\n",
        "        index += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ujJ3s6WQe0E",
        "outputId": "39041f5a-9a78-48de-cd04-8c95470b3cf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Threshold for unknown words replacement: 3\n",
            "Total words: 16920\n",
            "Occurrences of '<unk>': 32537\n"
          ]
        }
      ],
      "source": [
        "vocabulary = '/content/drive/My Drive/ColabNotebooks/CSCI544_HW3/vocab.txt'\n",
        "with open(vocabulary, \"w\", encoding=\"utf-8\") as file:\n",
        "  # First line should be <unk> words\n",
        "  file.write(f\"<unk>\\t0\\t{filtered_counts['<unk>']}\\n\")\n",
        "  # Known words\n",
        "  for word, idx in vocab.items():\n",
        "    if word != \"<unk>\":\n",
        "      file.write(f\"{word}\\t{idx}\\t{filtered_counts[word]}\\n\")\n",
        "\n",
        "print(\"Selected Threshold for unknown words replacement: 3\")\n",
        "print(f\"Total words: {len(vocab)}\")\n",
        "print(f\"Occurrences of '<unk>': {filtered_counts['<unk>']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Model Learning"
      ],
      "metadata": {
        "id": "PhLbPcskGXar"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "39U9ZylSUYzg"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VJQSE5MLU5Xn"
      },
      "outputs": [],
      "source": [
        "# Create a Hidden Markov Model JSON\n",
        "\n",
        "hmm = '/content/drive/My Drive/ColabNotebooks/CSCI544_HW3/hmm.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sFhpR8NRVM00"
      },
      "outputs": [],
      "source": [
        "# Create dictionaries to track occurances of the transitions/states/emissions\n",
        "\n",
        "transitions = {}\n",
        "states = {}\n",
        "emissions = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AEvyGMJPVpBf"
      },
      "outputs": [],
      "source": [
        "with open(train, \"r\", encoding=\"utf-8\") as file:\n",
        "    previous_state = None\n",
        "\n",
        "    for sentence in file:\n",
        "        sentence = sentence.strip()\n",
        "        if sentence:\n",
        "            _, word, state = sentence.split(\"\\t\")\n",
        "\n",
        "            # Increase the emissions dictionary count\n",
        "            if (state, word) in emissions:\n",
        "                emissions[(state, word)] += 1\n",
        "            else:\n",
        "                emissions[(state, word)] = 1\n",
        "\n",
        "            # Increase the states dictionary count\n",
        "            if state in states:\n",
        "                states[state] += 1\n",
        "            else:\n",
        "                states[state] = 1\n",
        "\n",
        "            # Increase the transitions dictionary count\n",
        "            if previous_state is not None:\n",
        "                if (previous_state, state) in transitions:\n",
        "                    transitions[(previous_state, state)] += 1\n",
        "                else:\n",
        "                    transitions[(previous_state, state)] = 1\n",
        "\n",
        "            # Make current state the previous state\n",
        "            previous_state = state\n",
        "\n",
        "        else:\n",
        "            prev_state = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EPWDA1U3XSrn"
      },
      "outputs": [],
      "source": [
        "# Create dictionaries to keep track of probabilities\n",
        "\n",
        "transition_probabilities = {}\n",
        "emission_probabilities = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Sh2hhS7uXZn5"
      },
      "outputs": [],
      "source": [
        "# Get probability by dividing count of state by total states\n",
        "for (state, next_state), count in transitions.items():\n",
        "  proportion = count / states[state]\n",
        "  transition_probabilities[(state, next_state)] = proportion\n",
        "\n",
        "# Get probability by dividing count of emission by total states\n",
        "for (s, e), count in emissions.items():\n",
        "  emission_proportion = count / states[s]\n",
        "  emission_probabilities[(s, e)] = emission_proportion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pPDx46AQXwa9"
      },
      "outputs": [],
      "source": [
        "# Adding the transtion and emission data to a dictionary to add to the JSON\n",
        "hidden_markov = {\n",
        "    \"transition\": {f\"{state},{next_state}\": probability for (state, next_state), probability in transition_probabilities.items()},\n",
        "    \"emission\": {f\"{s},{e}\": probability for (s, e), probability in emission_probabilities.items()}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAhIVlbTX5Kl",
        "outputId": "d4d43227-bf74-491c-a780-17da7a94aa8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Location the model has been saved to /content/drive/My Drive/ColabNotebooks/CSCI544_HW3/hmm.json\n",
            "Total transition parameters: 1378\n",
            "Total emission parameters: 50286\n"
          ]
        }
      ],
      "source": [
        "# Save the Hidden Markov Model as a JSON\n",
        "with open(hmm, \"w\", encoding=\"utf-8\") as file:\n",
        "    json.dump(hidden_markov, file, indent=4)\n",
        "\n",
        "# Output summary\n",
        "print(f\"Location the model has been saved to {hmm}\")\n",
        "print(f\"Total transition parameters: {len(transition_probabilities)}\")\n",
        "print(f\"Total emission parameters: {len(emission_probabilities)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Greedy Decoding"
      ],
      "metadata": {
        "id": "JNzVfMhiLJNL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WbZjsbnmbIPI"
      },
      "outputs": [],
      "source": [
        "# Greedy Decoding Algorithm\n",
        "def greedy_decoding(sentence):\n",
        "    predictions = []\n",
        "    prev_state = None\n",
        "\n",
        "    for i, word in enumerate(sentence):\n",
        "        best_state = None\n",
        "        best_prob = 0\n",
        "\n",
        "        for state in states:\n",
        "            # Get emission probability\n",
        "            emission_prob = emission_probabilities.get((state, word))\n",
        "\n",
        "            # If word is not in emission probabilities it goes to unknown\n",
        "            if emission_prob is None:\n",
        "                emission_prob = emission_probabilities.get((state, \"<unk>\"), 1e-6)\n",
        "\n",
        "            # If first word, probability set to 1\n",
        "            if i == 0:\n",
        "                transition_prob = 1\n",
        "            else:\n",
        "\n",
        "              # Retrieve probability of this transition\n",
        "                transition_prob = transition_probabilities.get((prev_state, state), 1e-6)\n",
        "\n",
        "            # Likelihood of word is product of transition and emission\n",
        "            prob = transition_prob * emission_prob\n",
        "\n",
        "            # Check if most likely\n",
        "            if prob > best_prob:\n",
        "                best_prob = prob\n",
        "                best_state = state\n",
        "\n",
        "        predictions.append((word, best_state))\n",
        "        # Greedy algorithm selects best state to remember\n",
        "        prev_state = best_state\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Vror5Qt1doEd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3wiyQgs2M4N",
        "outputId": "c6eee3db-67d5-4deb-a636-1ae771d38971"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved in /content/drive/My Drive/ColabNotebooks/CSCI544_HW3/greedy.out\n"
          ]
        }
      ],
      "source": [
        "output_test = \"/content/drive/My Drive/ColabNotebooks/CSCI544_HW3/greedy.out\"\n",
        "with open(test, \"r\", encoding=\"utf-8\") as infile, open(output_test, \"w\", encoding=\"utf-8\") as outfile:\n",
        "    sentence = []\n",
        "\n",
        "    for line in infile:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            index, word = line.split(\"\\t\")  # Test data has no POS tag\n",
        "            sentence.append((index, word))\n",
        "        else:\n",
        "            # Perform greedy decoding\n",
        "            predicted_tags = greedy_decoding([w for _, w in sentence])\n",
        "\n",
        "            # Write predictions in the same format as training data\n",
        "            for (index, word), (_, predicted_tag) in zip(sentence, predicted_tags):\n",
        "\n",
        "                outfile.write(f\"{index}\\t{word}\\t{predicted_tag}\\n\")\n",
        "\n",
        "            # Separate sentences\n",
        "            outfile.write(\"\\n\")\n",
        "\n",
        "            # Reset for next sentence\n",
        "            sentence = []\n",
        "\n",
        "print(f\"Predictions saved in {output_test}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-hlldaggslg",
        "outputId": "1c83ae65-753c-4081-9447-76a0a86be303"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved in /content/drive/My Drive/ColabNotebooks/CSCI544_HW3/dev_greedy.out\n",
            "total: 131768, correct: 123287, accuracy: 93.56%\n"
          ]
        }
      ],
      "source": [
        "# Setting location to output greedy predictions\n",
        "output = \"/content/drive/My Drive/ColabNotebooks/CSCI544_HW3/dev_greedy.out\"\n",
        "\n",
        "# Checking greedy decoding on test data\n",
        "with open(dev, \"r\", encoding=\"utf-8\") as infile, open(output, \"w\", encoding=\"utf-8\") as outfile:\n",
        "    sentence = []\n",
        "    indexes = []\n",
        "\n",
        "    for line in infile:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            index, word, _ = line.split(\"\\t\")\n",
        "            indexes.append(index)\n",
        "            sentence.append((index, word))\n",
        "        else:\n",
        "            # Use greedy algorithm to predict tags\n",
        "            predicted_tags = greedy_decoding([w for _, w in sentence])\n",
        "\n",
        "            # Write predictions in the same format as training data\n",
        "            for (index, word), (_, predicted_tag) in zip(sentence, predicted_tags):\n",
        "                outfile.write(f\"{index}\\t{word}\\t{predicted_tag}\\n\")\n",
        "            outfile.write(\"\\n\")\n",
        "\n",
        "            # Reset for next sentence\n",
        "            sentence = []\n",
        "            indexes = []\n",
        "\n",
        "    if sentence:\n",
        "      predicted_tags = greedy_decoding([w for _, w in sentence])\n",
        "\n",
        "    # Write predictions in the same format as training data\n",
        "    for (index, word), predicted_tag in zip(sentence, predicted_tags):\n",
        "      outfile.write(f\"{index}\\t{word}\\t{predicted_tag}\\n\")\n",
        "    outfile.write(\"\\n\")\n",
        "\n",
        "print(f\"Predictions saved in {output}\")\n",
        "!python \"/content/drive/My Drive/ColabNotebooks/CSCI544_HW3/eval.py\" -p \"/content/drive/My Drive/ColabNotebooks/CSCI544_HW3/dev_greedy.out\" -g \"/content/drive/My Drive/ColabNotebooks/CSCI544_HW3/dev\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kxsMX2MQ7poA"
      },
      "outputs": [],
      "source": [
        "with open(hmm, \"r\", encoding=\"utf-8\") as file:\n",
        "    hmm_model = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "AnrtRFLB71Sr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "transition_probs = {tuple(k.split(',')): v for k, v in hmm_model[\"transition\"].items()}\n",
        "emission_probalities = {tuple(k.split(',')): v for k, v in hmm_model[\"emission\"].items()}\n",
        "\n",
        "# Extract Part of Speech tag states\n",
        "states = list(set(s for s, _ in emission_probabilities.keys()))\n",
        "\n",
        "def viterbi_decoding(sentence):\n",
        "    n = len(sentence)\n",
        "\n",
        "    num_states = len(states)\n",
        "\n",
        "    # Initialize Viterbi and backpointer tables\n",
        "    viterbi = np.zeros((num_states, n))  # Probability table\n",
        "    backpointer = np.zeros((num_states, n), dtype=int)  # Best previous state index\n",
        "\n",
        "    # Convert states to index mapping\n",
        "\n",
        "    state_to_idx = {state: i for i, state in enumerate(states)}\n",
        "\n",
        "    # Initialize first column of Viterbi table\n",
        "    for i, state in enumerate(states):\n",
        "        emission_prob = emission_probabilities.get((state, sentence[0]), emission_probabilities.get((state, \"<unk>\"), 1e-6))\n",
        "        viterbi[i, 0] = emission_prob  # Assume uniform initial probability\n",
        "\n",
        "    # Fill Viterbi table for t > 1\n",
        "    for t in range(1, n):  # Loop through words in sentence\n",
        "        for i, curr_state in enumerate(states):\n",
        "            max_prob = -1\n",
        "            best_prev_state = 0\n",
        "\n",
        "            emission_prob = emission_probabilities.get((curr_state, sentence[t]), emission_probabilities.get((curr_state, \"<unk>\"), 1e-6))\n",
        "\n",
        "            for j, prev_state in enumerate(states):\n",
        "                transition_prob = transition_probabilities.get((prev_state, curr_state), 1e-6)\n",
        "                prob = viterbi[j, t-1] * transition_prob * emission_prob\n",
        "\n",
        "                if prob > max_prob:\n",
        "                    max_prob = prob\n",
        "                    best_prev_state = j\n",
        "\n",
        "            viterbi[i, t] = max_prob\n",
        "            backpointer[i, t] = best_prev_state\n",
        "\n",
        "    # Backtrace to find best path\n",
        "    best_final_state = np.argmax(viterbi[:, n-1])\n",
        "    best_path = [best_final_state]\n",
        "\n",
        "    for t in range(n-1, 0, -1):\n",
        "        best_final_state = backpointer[best_final_state, t]\n",
        "        best_path.insert(0, best_final_state)\n",
        "\n",
        "    # Convert indices back to states\n",
        "    best_tags = [states[i] for i in best_path]\n",
        "    return best_tags"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "viterbi_test_file = \"/content/drive/My Drive/ColabNotebooks/CSCI544_HW3/viterbi.out\"\n",
        "with open(test, \"r\", encoding=\"utf-8\") as infile, open(viterbi_test_file, \"w\", encoding=\"utf-8\") as outfile:\n",
        "    sentence = []\n",
        "    indexes = []\n",
        "\n",
        "    for line in infile:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            tokens = line.split(\"\\t\")\n",
        "            if len(tokens) == 2:  # Ensure correct format (test data has no true labels)\n",
        "                index, word = tokens\n",
        "                indexes.append(index)\n",
        "                sentence.append(word)\n",
        "        else:\n",
        "            # Perform Viterbi decoding\n",
        "            predicted_tags = viterbi_decoding(sentence)\n",
        "\n",
        "            # Write predictions following the same order\n",
        "            for index, word, predicted_tag in zip(indexes, sentence, predicted_tags):\n",
        "                outfile.write(f\"{index}\\t{word}\\t{predicted_tag}\\n\")\n",
        "            outfile.write(\"\\n\")  # Keep blank lines between sentences\n",
        "\n",
        "            # Reset for next sentence\n",
        "            sentence = []\n",
        "            indexes = []\n",
        "\n",
        "print(f\"Test data predictions saved in {viterbi_test_file}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Phk10G5dsSHv",
        "outputId": "280fd17a-99b2-4fa8-933e-9e323650df0e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test data predictions saved in /content/drive/My Drive/ColabNotebooks/CSCI544_HW3/viterbi.out.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H23o2J2d73Re",
        "outputId": "2700e064-145f-47e5-8e0c-0ac09006297d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total: 131768, correct: 124857, accuracy: 94.76%\n"
          ]
        }
      ],
      "source": [
        "viterbi_file = \"/content/drive/My Drive/ColabNotebooks/CSCI544_HW3/dev_viterbi.out\"\n",
        "with open(dev, \"r\", encoding=\"utf-8\") as infile, open(viterbi_file, \"w\", encoding=\"utf-8\") as outfile:\n",
        "    sentence = []\n",
        "    indexes = []\n",
        "\n",
        "    for line in infile:\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            tokens = line.split(\"\\t\")\n",
        "\n",
        "            #Check for correct length\n",
        "            if len(tokens) == 3:\n",
        "\n",
        "                # Append to lists\n",
        "                index, word, _ = tokens\n",
        "                indexes.append(index)\n",
        "                sentence.append(word)\n",
        "        else:\n",
        "            # Perform Viterbi decoding\n",
        "            predicted_tags = viterbi_decoding(sentence)\n",
        "\n",
        "            # Write predictions following the same order\n",
        "            for index, word, predicted_tag in zip(indexes, sentence, predicted_tags):\n",
        "              outfile.write(f\"{int(index)}\\t{word.strip()}\\t{predicted_tag.strip()}\\n\")\n",
        "            outfile.write(\"\\n\")  # Keep blank lines between sentences\n",
        "\n",
        "            # Reset for next sentence\n",
        "            sentence = []\n",
        "            indexes = []\n",
        "    if sentence:\n",
        "      # Perform Viterbi decoding on the last sentence\n",
        "      predicted_tags = viterbi_decoding(sentence)\n",
        "\n",
        "    # Write predictions for the last sentence\n",
        "    for index, word, predicted_tag in zip(indexes, sentence, predicted_tags):\n",
        "        outfile.write(f\"{int(index)}\\t{word.strip()}\\t{predicted_tag.strip()}\\n\")\n",
        "    outfile.write(\"\\n\")  # Keep blank lines between sentences\n",
        "\n",
        "!python \"/content/drive/My Drive/ColabNotebooks/CSCI544_HW3/eval.py\" -p \"/content/drive/My Drive/ColabNotebooks/CSCI544_HW3/dev_viterbi.out\" -g \"/content/drive/My Drive/ColabNotebooks/CSCI544_HW3/dev\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}